% !TEX root = ../maturaarbeit.tex
\newpage
\chapter{Solutions with Policy Gradient Methods: WORK IN PROGRESS}\label{chap:policy_gradient}
\noindent
In this section I shall briefly present Policy Gradient methods, their advantages and aim to give an intuitive understanding of their operation to the reader. Policy Gradients (PG) are the basis of the algorithm used in this thesis to solve the soccer problem. They are one of the main methods used in modern Reinforcement Learning. Countless algorithms build on them, one of which I will present and use here.

\section{What are Policy Gradient Methods?}\label{pg:what_are_policy_gradient_methods}
Policy Gradient Methods Rely on the concept of the policy as a function which produces a probability distribution over the action space, and the concept of gradient ascent, which i discussed in the previous chapter \ref{gd:gradient_ascent}. At the end of that chapter I mentioned how Gradient Descent / Ascent could be used in Learning, By defining a Performance Measure dependant on the policy's parameters, and then differentiating that with respect to those parameters, thereby finding out in what direction to change them in order to improve the policy. Here I will present just such a measure, denoted as $J(\theta)$. Conceptually $J(\theta)$ is the true value of of $\pi$ with the parameters $\theta$ starting in a state $s_0$. It can be thought of as a measure of how well an agent following the policy will perform starting at $s_0$. 

\section{Why I chose Policy Gradient Methods for my Work}\label{pg:reasons_for_pg}
\noindent
Regrettably the field of Reinforcement Learning is far too broad to cover all of its approaches here, and thus laying out the advantages of Policy Gradient methods over other approaches is inevitably limited by the limited context available. 

\section{The Policy Gradient Theorem}\label{pg:theorem}
In \citepg{324} $J(\theta)$ it is defined as follows:

\begin{equation}\label{pg:performance_measure}
    J(\theta) \doteq v_{\pi_\theta}(s_0)
\end{equation}
\noindent
where $v_\theta(s_0)$ is:
\begin{equation}\label{pg:true_value_function}
    v_{\pi_\theta}(s) \doteq \mathbb{E}_{\pi_\theta}[G_t|S_t=s]
\end{equation}
\centerline{\small\textit{from \citepg{58}}}

\noindent
\\ I will not be using $v_{\pi_\theta}(s)$ in my thesis, it extends beyond the scope of my work, especially as it pertains to the practical part. I bring it up here because it is used in the definition of $J(\theta)$ in the Policy Gradient Theorem \ref{pg:theorem}