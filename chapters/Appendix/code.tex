% !TEX root = ../maturaarbeit.tex

\chapter{Code}\label{appendix:code}
\section{Move Agent Method}\label{appendix:code:move_agent}
\begin{lstlisting}[basicstyle=\footnotesize]
public void MoveAgent(ActionSegment<float> act) {
    m_KickPower = 0f;

    float longitudinalAxis = Convert.ToSingle(System.Math.Tanh(act[0])) * m_ForwardSpeed;
    float lateralAxis = Convert.ToSingle(System.Math.Tanh(act[1])) * m_LateralSpeed;
    float rotationalAxis = Convert.ToSingle(System.Math.Tanh(act[2]));

    Transform transformTMP;
    (transformTMP = transform).Rotate(transform.up, rotationalAxis * 6.5f);
    agentRb.AddForce(
        (transformTMP.forward * longitudinalAxis +
         transformTMP.right * lateralAxis)
        * m_SoccerSettings.agentRunSpeed,
        ForceMode.VelocityChange);
}
\end{lstlisting}

\section{Create Policy Network}\label{appendix:code:network}
\begin{lstlisting}[basicstyle=\footnotesize]
def get_size_decaying_policy_net(
        observation_shape,
        action_shape,
        kernel_initializer: K.initializers.Initializer = K.initializers.HeNormal(seed=0),
        final_continuous_initializer: K.initializers.Initializer = K.initializers.RandomNormal(0, 0.05),
        activation=K.activations.relu,
        bias_mu: float = 0.0,
        bias_sig: float = 0.0,
        sig_single_var: bool = True) -> K.Model:

    node_count = max(int(2 ** np.floor(np.log2(np.sum(observation_shape)))), 64)
    min_node_count = max(int(2 ** np.ceil(np.log2(np.sum(action_shape)))), 32)
    node_count = max(64, min_node_count * 4) if node_count <= min_node_count else node_count

    inputs = K.Input(int(np.sum(observation_shape)))
    dense_layers = [K.layers.Dense(node_count, activation=activation, kernel_initializer=kernel_initializer, name=f'dense_{node_count}')(inputs)]
    node_count /= 2
    layer = 0
    
    while node_count >= min_node_count:
        dense_layers.append(K.layers.Dense(node_count, activation=activation, kernel_initializer=kernel_initializer, name=f'mu_dense_{node_count}')(dense_layers[layer]))
        node_count /= 2
        layer += 1
    mu_out = K.layers.Dense(action_shape, name='mu_out', kernel_initializer=final_continuous_initializer, bias_initializer=K.initializers.constant(bias_mu))(dense_layers[-1])
    
    sig_pre = IndependentTrainableVarLayer('sigpre', K.initializers.constant(bias_sig), action_shape)(inputs)  # it does not matter what this is called on
    sig_out = tf.exp(sig_pre, name='sig_out')  # sigmas in log space so no negative values

    return K.Model(inputs=inputs, outputs=[mu_out, sig_out])

\end{lstlisting}

\section{Compute Returns}\label{appendix:code:returns}
\begin{lstlisting}[basicstyle=\footnotesize]
@staticmethod
def get_fully_discounted_returns(rewards: tf.Tensor, gamma: float):
    horizon = rewards.shape[0]
    gamma = tf.ones([horizon, horizon], tf.float32) * gamma
    exponent = tf.range(horizon, dtype=tf.float32) - tf.expand_dims(tf.range(horizon, dtype=tf.float32), 1)
    mask = tf.pow(gamma, exponent)
    mask = tf.linalg.band_part(mask, 0, -1)  
    returns = rewards * mask
    returns = tf.reduce_sum(returns, axis=1, keepdims=True)
    return returns
\end{lstlisting}

\section{Trajectory Class}\label{appendix:code:trajectory}
\begin{lstlisting}[basicstyle=\footnotesize]
class Trajectory:
    def __init__(self, agent_id, max_len=1000):
        self.max_len = max_len
        self.id = agent_id
        self.states = deque()
        self.actions = deque()
        self.rewards = deque()
        self._out_transitions_states = []
        self._out_transitions_actions = []
        self._out_transitions_returns = []
        self._out_transitions = []
        self._running_idx = 0

    @property
    def full(self):
        return self._running_idx >= self.max_len

    def push(self, state: ndarray, action: ndarray, prev_reward: float): 
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(prev_reward)

    def finalize(self, final_reward: float, gamma: float):
        diff = self.max_len - self._running_idx
        len_idx = len(self.states) if diff >= len(self.states) else diff
        if self.full:
            raise Exception("trajectory already full, please handle first!!!")
        self.rewards.append(final_reward)
        self._out_transitions_states.append(tf.concat(self.states, 0)[:len_idx])
        self._out_transitions_actions.append(tf.concat(self.actions, 0)[:len_idx])
        self._out_transitions_returns.append(
            self.get_fully_discounted_returns(
                (tf.convert_to_tensor(self.rewards)[1:]), gamma=gamma)[:len_idx])
        self._running_idx += len_idx
        self.flush()

    def pop_transitions(self):
        return (tf.concat(self._out_transitions_states, int(0)),
                tf.concat(self._out_transitions_actions, int(0)),
                tf.concat(self._out_transitions_returns, int(0)))

    def flush(self): 
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()

    def purge(self):
        self.flush()
        self._out_transitions_states.clear()
        self._out_transitions_actions.clear()
        self._out_transitions_returns.clear()
        self._running_idx = 0
\end{lstlisting}

\section{Manager Class}\label{appendix:code:env_manager}
\begin{lstlisting}[basicstyle=\footnotesize]
EnvId = int  


class Manager:
    def __init__(self, env_count: int, env_path: str, render: bool = False):
        self.env_count = env_count
        offset = np.random.randint(0, 1000)
        self.envs: Dict[EnvId, UnityEnvironment] = {
            env_id: UnityEnvironment(
                file_name=env_path,
                worker_id=env_id + offset,
                seed=int(np.random.randint(0, 2_147_483_648)),
                side_channels=[],
                no_graphics=not render
                # additional_args=["-batchmode"] if render else []
            ) for env_id in range(env_count)
        }
        self.offset = 0
        for env_id, env in self.envs.items():
            env.reset()
            self.offset += sum(len(env.get_steps(name)[0]) for name in env.behavior_specs.keys())

        self.action_queue = JoinableQueue(maxsize=self.env_count)
        self.step_queue = JoinableQueue(maxsize=self.env_count)
        self.worker_pool = Pool(initializer=self._worker)
        self.reset_lock = Lock()
        self.current: EnvId = 0
        self.env_id_dupe_checker_debug = {env_id: 0 for env_id in self.envs.keys()}

    @property
    def behaviour_specs(self):
        return self.envs[0].behavior_specs.items()

    def agent_ids(self, behaviour_name: str):
        self.explicit_reset()
        ids = []
        for env_id, env in self.envs.items():
            decision_steps, _ = env.get_steps(behaviour_name)
            decision_steps.agent_id += self.offset * env_id
            ids.extend(decision_steps.keys())
        return ids

    def _worker(self):
        while True:
            # Get a "work item" out of the queue.
            env_id = self.step_queue.get()
            self.env_id_dupe_checker_debug[env_id] -= 1
            self.envs[env_id].step()
            self.action_queue.put(env_id)
            self.step_queue.task_done()

    def acquire(self):
        self.current = self.action_queue.get()
        self.env_id_dupe_checker_debug[self.current] += 1
        if self.env_id_dupe_checker_debug[self.current] != 1: raise Exception('bad concurrency')
        return self.current

    def release(self):
        self.action_queue.task_done()
        self.step_queue.put(self.current)

    def soft_reset(self):
        self.step_queue.join()
        with self.step_queue.mutex:
            self.step_queue.queue.clear()
        with self.action_queue.mutex:
            self.action_queue.queue.clear()
        for env_id, env in self.envs.items():
            self.action_queue.put(env_id)

    def explicit_reset(self):
        self.soft_reset()
        for env in self.envs.values():
            env.reset()

    def get_steps(self, behaviour_name: str) -> Tuple[DecisionSteps, TerminalSteps]:
        decision_steps, terminal_steps = self.envs[self.current].get_steps(behaviour_name)
        decision_steps.agent_id += self.offset * self.current
        terminal_steps.agent_id += self.offset * self.current
        return decision_steps, terminal_steps

    def set_actions(self, behaviour_name: str, actions: np.ndarray):
        self.envs[self.current].set_actions(behaviour_name, actions)

\end{lstlisting}