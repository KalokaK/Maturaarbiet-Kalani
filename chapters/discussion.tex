\chapter{Discussion}\label{chap:discussion}
\section{Evaluating Data}\label{sec:disc:evaluation}
Evaluating training data posed an unexpected challenge, especially when polices compete against themselves, in the terminology used in \cite{juliani2020unity} "self play" is involved. In other machine learning tasks such as classification, a clear right or wrong answer may be ascribed to the computers performace, notions of "good gameplay" even differ vastly between humans. This is complicated by the fact that in self play tasks such as "Soccer", not even the return value can be relied upon to measure agent performance. Evaluation of all variations of an agent agaisnt some common baseline without testing between agents is also flawed, as its possible that an agent which performs poorly against all others, excels against the baseline. I considered evaluation against a human agent, but not only would two human plays be needed which may differ greatly in skill, but for data against human agents to be remotely significant would require thousands of games, this is simply impractical. The elo rating system is an incredibly valuable tool and if sample count is high can produce meaningful results, it is not a perfect measure of agent performance against a specific opponent, but an overview of performance against all others. For absolute performance between two parameters, the win probabilities between agents trained with them must compared, I do so when elo ratings are close.


\section{Statistical Significance}\label{sec:disc:significance}
% DO INTRO 
%To evalue training outcome it is necessary to know how meaningful the data obtained is. Various aspects introduce error which I accounted for as best I could. 
Here I go over how error rates were determined and the unaccounted for uncertainty I am aware of The Error in percentage of wins of one parameter over another introuduced by evaluation is fairly low, game outcomes between agents are technically multinomial distributions but because ties are so infrequent (on average $\approx$ 0.5\% of outcomes), and all my data for win probabilities is close to 0.5, i use the "Wade" confidence interval of a binomial\cite[p. 2]{binomial_confidence} to approximate it. I accept a high error level\footnote{$1-\alpha$ can be thought of as the confidence that data falling outside the interval obtained with it, is meaningful.} of $\alpha = 0.2$, as this thesis' focus lies not in perfect parameters, but generally agent training. The error in win probability obtained through this method turns out to be $\approx$0.013\footnote{$p \pm e, e = z\sqrt{p(1-p)/n}$ z is the number of standard deviations within which the percentage of data specified by $\alpha$ falls \cite[p. 2]{binomial_confidence}. It can be obtained by solving $1-\alpha = 2\int_0^z \sqrt{2\pi}^{-1}\exp(-t^2/2)dt$ for $z$.} which corresponds to 1.3\% for all parameters. This corresponds to a difference in elo between parameters of approximately 9\footnote{$\Delta elo = \log_{10}\left(\left(p_{win}^{-1}-1\right)^{-400}\right)$, this increases by $\approx$ 9.0 $\pm$ 0.5 for a percent error of 1 if $p_{win} = 0.5$ $\pm$ 0.12 which all of my data falls in to.} points. The more troublesome error is in the difference between between runs. I cannot control the random seed used for Unity, which makes 1 to 1 repeatability of runs impossible. Monte Carlo approximation of actual parameter performance is prohibitively expensive, instead I estimate the variance in runs as

\begin{equation*}
    \sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( x_i - \overline{x} \right)^2
\end{equation*}

\noindent
and add to the variance in the binomial confidence interval. The variance of 8 sample runs at 10 million steps is $\approx$ 0.00035, corresponding to an error of $0.028 = 1.3\cdot \sqrt{p(1-p)/2560 + 0.00035}$ if $p_{win} = 0.5 \pm 0.12$, this holds for all my data. however this again imperfect as all data is interdependant. This can be mitigated by letting all sample agent's play against a common opponent, and using the average win percentage of the results for the variance, this methods yields: $\sigma^2 \approx 0.00010$, leading to an error of 0.018, again with $p_{win}$ = 0.5. This is perhaps more representative data as it more accurately reflects the scenario of comparison between parameters, it is the value which I will use, it corresponds to a $\Delta elo$ between parameters of $\approx 13.4$. Further, 20 million time steps might not be fully representative of agent performance after 100 million. For me, the computational cost of full training runs is simply too high. 



\iffalse
\section{Training Process}
 Agent training for even a relatively simple problem such as this soccer simulation incurs immense computational and temporal cost, including initial testing, though much of this was in parallel, total training time during this thesis including initial testing was roughly 250 hours. Some of this was due to inefficiencies in my methodology, and perhaps poor code performance. As the training algorithm gets more complex and additional variables are introduced, this issue only worsens. 
 \\ Evaluation of training data is also quite difficult, especially in tasks where self play is involved. The elo rating system is a decent indicator of overall performance, but might not give the best parameter choice when testing.
 \fi



\section{Outlook}
Herein I will briefly cover possible future work. In terms of complexity my training algorithm is quite minimalist. The return term of the policy gradient may be substituted by Function approximation and more complex methods, which have been shown to result in considerably higher performance across an array of tasks. Some of these are discussed in \cite{arulkumaran_brief_2017}, which gives an overview of modern reinforcement learning. Further I originally intended to explore computer vision based observations, which have become viable due to advances in methodology and hardware performance over the last decade. I consider pursuing this path worthwhile because then direct comparison to human play would be significantly more meaningful, as observations could essentially be equivalent. Another interesting avenue of research would be communication channels between agents. I do not know how compliant this would be with the model of a Markov Decision Process, but I think that some form of communication between agents could lead to much higher performance, one of the main sources of bad play seems to still be the "infighting" behaviour mentioned in \ref{sec:tr:baseline_results}. Another approach to dealing with "infighting" might be to treat an entire team as one agent, then a system where the actions for each player in a team are produced by an independent neural network, and rewards generated by a "manager". This could lead to specialization within the "team" agent, procedurally akin to how different neurons in a neural network encode information.
Lastly, I found data evaluation in this task to be unrefined for lack of a better term. This is mostly because of the lack of an objective measurement available in tasks where "self play" is used. It is possible if not likely, that such a measure exists and I simply am unaware of it, however, if this is not the case then developing systems as to better evaluate training data might lead to more significant results.