% !TEX root = ../maturaarbeit.tex
\newpage
\chapter{Function approximation using Artificial Neural Networks: WORK IN PROGRESS}\label{nn}
In previous Chapters I relied heavily on policies being a possibly parameterized function, that is on policies as a mapping from a state to a probability distribution over the action space. An optimal policy would thereby a function which optimally maps each state to the best possible distribution over the action space. We do not know that function, but using methods like the ones shown in \ref{chap:policy_gradient} we can optimize the parameters of one which approximates it. Artificial Neural Networks offer A method of approximating complex functions with useful results, and importantly are quite scalable, hence they are often used in Machine- and Reinforcement Learning. Another reason why I and many others use them, is that, as stated in \citepg{33}, using neural networks a reinforcement learning algorithm can generalize from previous states. This means that a policy involving neural networks can be trained on a limited subset of the states and perform decently in all of them. This is crucial to training in practice, where some states are rarely visited and agents cannot be trained indefinitely. Artificial Neural Networks date back to the 1960s, early concepts even to the 1940s. I will only explain of many variations of them. 

\section{Components of a Neural Network}\label{nn:components}
The Type of Neural Network I will be using is called a Multilayer Perceptron (MLP). Similar to a real Neural Network, a MLP is made up of \textit{Neurons}. A Neuron takes input from several other neurons and produces an output signal. In MLPs These Neurons are arranged in layers. In a MLP the input, in my case an observation of the current state, is passed to the first layer and the output, in my case the parameters for the distribution over the actions from which I sample, is taken from the last layer. In between these are arranged sequentially. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/placeholder.png}
    \caption{Multi Layer Perceptron for Policy Gradient Methods}
    \label{fig:my_label}
\end{figure}

\noindent
The above image illustrates how all these components are arranged to form a network. How each of these components function, I will explain in the following sections.

\subsection*{Neurons}\label{nn:neurons}
In an MLP a neuron takes an input vector $\nvec{x} \in \mathbb{R}^j$ which corresponds to the outputs of the neurons in the last layer, and multiplies each of its elements by a scalar \textit{weight} $w_j$. A neurons output $z$ is the sum of all of these products. This weight vector is what parameterizes the neurons output. It's elements are what is updated during gradient descent.

\begin{equation}\label{nn:neuron_z}
    z = \sum_{j}x_j w_j
\end{equation}

\noindent
\\ Notice how this is a linear transformation , the weight vector $\nvec{w} = \{ w_0, \dots, w_j \}$, applied to the input vector $\nvec{x} = \{ x_0, \dots, x_j\}$. it can be denoted as $\nvec{w}^T \nvec{x}$. A neuron is connected to another if it uses its output in computing its own.

\subsection*{Dense Neuron Layers}\label{nn:layers}
A Dense, or fully connected \textit{layer}, is a group of neurons which all take a common previous layer as input. It is fully connected, because each neuron is connected to all neurons in the previous layer. Other layers might for example concatenate two previous ones. All though only dense layers are present in an MLP, I will still use the terminology, to avoid confusion when introducing external tools I use in the practical application. Every Neuron taking the full previous layer as input also has some implication for notation and computation. The same way computing $z$ for one neuron is applying a linear transformation , computing $z$ for all neurons is as well. If there are $i$ neurons in the current layer, and $j$ in the previous, a weight matrix $\mathbf{W} = [\nvec{w}_0 , \dots, \nvec{w}_i]^T$ which is in $\mathbb{R}^{i \times j}$ can be constructed from all $\nvec{w}_i$. If that matrix then is applied to an input vector $\nvec{x} \in \mathbb{R}^j$, we get a vector which corresponds to the weighted sum of inputs for all neurons in the current layer, this vector is $\nvec{z}$. Computing $z$ of each neuron in a dense layer is nothing but matrix vector multiplication! Therefor passing input from one layer to the next through the entire neural network is nothing more than sequentially applying linear transformations to an input vector $\nvec{x}$ to obtain an output. If $f_n(x,\textbf{W}_n)$ is the nth layer in an MLP, then its output would be $f_n (\dots f_2 (f_1 (x, \textbf{W}_1), \textbf{W}_2) \dots, \textbf{W}_n)$. The weight matricies are what parameterize an MLP's layers, and in turn the MLP itself. This notation can be shortened to $f(x, \theta)$, where theta is all the MLP's parameters.

\section*{Making Layers non Linear}\label{nn:activation}
If an MLP were only comprised of a series of linear transformations, Then the entire Network could be described in a single matrix, a series of linear transformations is itself a linear transformation. This property makes a network of only linear functions pointless, it introduces needlessly many parameters which need to be updated and it can never produce better results than a single linear function could. Additionally the accuracy with which linear functions can approximate others is very limited. Because of all this so called "activation functions" are used in between layers. An activation function takes z values and applies some non-linear transformation to them. A common one, and the one I use is ReLU, which is an acronym for 

\section{Multi Layer Perceptrons in the context of Policy Gradient Methods}\label{nn:example}
