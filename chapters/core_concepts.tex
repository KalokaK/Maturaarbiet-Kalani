% !TEX root = ../maturaarbeit.tex
\chapter{Core concepts of Reinforcement Learning}\label{chap:theory}
\section{What is Reinforcement Learning?}\label{sec:RL}
\noindent
Throughout our daily lives we navigate our surroundings, handle social situations and tackle complex tasks. In doing, so we take the actions we believe, based on experience and intuition, to have the best outcome. We might take these abilities for granted, given how natural they are to us. However, at some point we had to obtain these skills so essential in managing our day to day, many through simple trial and error. Reinforcement Learning (RL) seeks to formalize the process of figuring out how to behave based on what produces desirable results and what does not, and adjusting our future actions accordingly. 

\noindent
\\ Reinforcement Learning is a discipline of machine learning \cite[p. 1]{sutton_reinforcement_2018}, an incredibly broad field which is focused on the optimization of computer algorithms by processing data and experience. As such it is an intersection between the natural, to us intuitive concept of learning, and the rigid and numerical world of computer programming and mathematics \cite[p. 4]{sutton_reinforcement_2018}. To be able to understand how this learning process works, and to be able to quantify and formalize it, it is essential to introduce some general concepts. 

\subsection{Illustrative Example: Card game UNO}\label{subsec:UNO}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/UNO.png}
    \caption{UNO environment in the context of RL}
    \label{fig:UNO}
\end{figure}

\noindent
Consider the example of teaching someone how to play the popular card game “UNO”. The \textit{objective} of the game is to get rid of the cards on your hand before the other players. We can call the game an \textit{environment}. This environment contains all the players players, including their strategies, the cards and what rules the game follows. A state of an environment describes the arrangement of all components encompassed by it, in this case the hands of the players, who's turn it is, what cards are on which pile and their order. Notice that the environment contains a lot of information which the player, in Reinforcement Learning called an \textit{agent} (an actor in the environment) does not know. However, the actor can \textit{observe} the environment and thereby gain a reasonably accurate representation of it’s state. Suppose it is the agent's turn. Based on its \textit{observation} the agent can take an \textit{action a}, playing any of the cards on its hand, or picking one up. The rules of an environment might forbid certain actions, taking one of them, will lead to the same environment state, where it is the agents turn. A well designed environment will give the agent negative feedback, when teaching someone UNO, that might be telling them they did something wrong. In Reinforcement Learning this is called \textit{reward signal}. Based on the \textit{reward}, the agent then can update its behaviour to produce a different action next time. Thereby the reward \textit{reinforces} the desired behaviour, it is a \textit{reinforcement} the agent learns from. The way of acting of an agent in a state given, an observation of that state, is called a \textit{policy}, commonly denoted as $\pi$. The agent follows a policy to decide on an action, based on a state.
\newline
For the sake of brevity, unless the distinction is relevant, I am going to use the terms observation and state interchangeably. Much more expansive definitions going in to the nuances of these concepts can be found in the book \textit{Reinforcement Learning, An introduction} by Richard Sutton and Andrew Barto. \cite{sutton_reinforcement_2018}

\subsection{What Type of Problem does Reinforcement Learning suit?}\label{subsec:Why_RL}
When solving a problem where the environments rules are well known like UNO Reinforcement Learning might indeed not be the best approach. In the case of UNO, it might be more efficient to design an algorithm which computes some probability of victory for each action, based on what cards have been played and then picks the optimal one. However, this approach requires knowledge of how the environment operates \cite[p. 8]{sutton_reinforcement_2018}, if for example, it was not know what happens after our turn, it would suddenly become nigh impossible to explicitly define a probability of victory for an action. Another issue that quickly arises as the environment becomes more complex, is the rapid rise in the number of its possible permutations. Accounting for all of them quickly becomes unfeasible as the environment grows more complex. Reinforcement Learning lets us generate high quality solutions in uncertain environments based on a reward signal and the goal it ultimately describes \cite[p. 03]{sutton_reinforcement_2018}. Another approach which might come to mind as an obvious solution would be to simply mimic the behaviour of an optimal, or close to optimal, agent. However, this again is impractical as such an agent might simply not exist or generating sufficient examples can be tedious. As stated in Reinforcement Learning, An Introduction: “In uncharted territory—where one would expect learning to be most beneficial, an agent must be able to learn from its own experience.” \cite[p. 02]{sutton_reinforcement_2018} Here it is important to keep in mind that machines see environments differently from humans. For a human the actions involved in pouring a coffee, might be to pick up the can and tilt it. Plenty of examples exist on how to do that. However, we are only able to make sense of the example because we intuitively understand how move our limbs, the instructions of "pick up and tilt" would be useless to a robot which views the world through a grid of pixels and which can take only take actions in the form of powering a series of motors which move an arm, it would need an example which matches its way of interaction with the environment.

\section{The Finite Markov Decision Processes, a mathematical framework for Reinforcement Learning}\label{sec:MDP} % NOTE: THESE ARE ALL REFERENCES TO THE BOOK
I this section I will introduce Finite Markov Decision Processes (MDPs) which serve as a formalization of sequential decision making. Problems which can be described as a finite MDP are what Reinforcement Learning is trying to solve. They are called finite because the sets $\mathset{S}, \mathset{A}, \mathset{R}$ of all states, actions, and rewards respectively, are all finite. I will illustrate and apply the concepts in this section using a Grid-World environment. It serves as a particularly convenient example since in it, the state of the environment is fully described by the agent's position. This allows for simple computation and visualization. 

\subsection{Sequential decision making}\label{subsec:sequential_decision_making}
In an MDP the agent and environment continually interact. At each time step $t$ of that interaction, the agent selects an action, the environment transitions in to a new state $S_{t+1}$ and gives the reward $R_{t+1}$. Based on this new state, the agent selects another action. This series of interactions may go on forever. From this there also follows that states can be revisited, a finite set of states could not support an infinite series of transitions otherwise. $S_ {t+1}$ is just the state which the environment transitions to next and does not refer to a specific state in the set of states, the environment may even transition back to itself, in that case $S_t$ and $S_{t+1}$ would be identical. This is possible because all possible futures solely depend on that state, else it would not fully describe the environment.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/agent_environment_interaction_loop.png}
    \caption{Agent Environment Interaction Loop as presented in \citepg{48}}
    \label{fig:agent_env_inter}
\end{figure}

\noindent
Since the agent environment interaction is sequential, a sequence, or in Reinforcement Learning \textit{trajectory} $\tau$, arises. \citepg{48}

\begin{equation}\label{MDP:trajectory}
    \Tau = S_0, A_0, R_1, S_1, A_1, R_2 \dots
\end{equation}
\centerline{\small\textit{from \citepg{48}}}

\noindent
\\ In an \ita{episodic} environment, an environment that ends at some terminal time-step $T$, a trajectory has a terminal state $S_T$ and reward $R_T$. The notion of a terminal action does not make sense since the environment terminates at $T$, any action taken would not have an effect, and the action which proceeds $S_T$ is $A_{T-1}$.

\subsubsection{Illustrating in Grid-World}\label{subsubsec:grid_world_trajectory}
The Agent starts at a starting position, here the bottom left corner, and has to reach the upper right corner, by choosing from the actions up, down, left, right at each time step. The blacked out fields are inaccessible, running in to them or the walls, just results in the same state, the agent does not move. The rewards are not yet displayed here to avoid clutter, they will be discussed in the next section.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Grid_world_trajectory.jpeg}
    \caption{The Grid World Environment}
    \label{fig:grid_world}
\end{figure}

\noindent
The agent then moves through the states and therein generates a trajectory. The representation of that trajectory overlaid on to the grid, works in this case because the agents position fully describes an environment state. Archiving an agents trajectory becomes useful when evaluating and training the agent, since an evaluation based upon a series of correlated events, is obviously more meaningful, than one based entirely on one singular action and its consequence.

\subsection{Agent Policies in a MDP}\label{subsec:policies}

Moving forward it is important to understand what policies are mathematically. So far it has been established that policies are the way in which an chooses an action based on a state. Policies do not have to be deterministic. Mathematically, the policy is a function which takes a state and produces a probability distribution over the set of actions. This function can be parameterized, the parameter vecotr is commonly written as $\theta$. This distribution can be discrete or continuous and may produce vector actions. An action can be obtained from a policy by sampling from it. This is outlined in \citepg{58}, where $\pi(a|s)$ is defined as the probability of $a$ given $s$ under $\pi$. In our example of grid world the policy would be a discrete distribution. If the policy is parameterized, then $\pi(a|s)$ becomes $\pi(a|s,\theta)$. 

\subsection{The episodes Return, an improved measure of agent performance}\label{subsec:goals}

The agent selects actions based on state information and and receives rewards as feedback for its previous action. The goal of the agent is to maximize the reward signal received. It does that by updating its policy to produce a better action $A_t$ in the state $S_t$ based on the signal received at $t$. This means that one of the ways in which we can improve learning performance is to tweak the reward signal. The perhaps most obvious signal to give the agent is the reward $R_{t+1}$ which followed its action. However this does not take in to account any of the future states and rewards the agent's action lead to, it would not at all plan for any future states and rewards. To curb this issue we introduce the \textit{return} $G_t$. In contrast to the reward, the return also considers the influence of $A_t$ on future rewards. It is defined as follows:

\begin{equation}\label{MDP:return}
    G_t \doteq R_t + R_{t+1} + R_{t+2} \dots + R_T
\end{equation}
\centerline{\small{\ita{\citepg{54}}}}

\noindent
\\ Although the return undoubtedly is a better measure of the "goodness" of an action than the reward and training the agent using it produces decent results, it still has some flaws. Of these the most relevant to my work is that taking the sum of the rewards from the current time step to the episode's end gives the same weight to all the rewards. In many environments $A_{t-600}$ is much less relevant to $R_{t+1}$ then $A_t$, yet $R_{t+1}$ has the same weight in $G_{t}$ as it has in $G_{t-600}$. To address this, the discounted return can be used. 

\begin{equation}\label{MDP:discounted_return}
    G_t = \sum_{k=t}^{T} R_{t+k+1} \cdot \gamma ^k 
\end{equation}
or recursively as
\begin{equation}\label{MDP:recursive_discounted_return}
    R_{t+1} + \gamma \cdot G_{t+2} \mathrm{\ where\ } 0 \leq \gamma \leq 1
\end{equation}
\centerline{\small{\ita{\citepg{55}}}}

\noindent
\\ By tweaking the discount factor we can balance immediate reward with future ones. It is another hyper parameter that needs to be set which affects agent performance. Discounting also enables the use of the return in continuous environments, it is why it was originally introduced and is the main reason it is presented in \cite{sutton_reinforcement_2018}. {\\ \color{red} SHOW RESULTS OF DISCOUNT FACTOR 1 VS 0.99 AND SAY THAT IT IS BAD} \\ The returns in Grid-World with different discount factors can be seen here:

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/grid_world_discount_factors.png}
    \caption{Returns for each time step in an episode for different discount factors}
    \label{fig:grid_world_discount_factors}
\end{figure}

\noindent
\\ The only reward the agent receives here occurs on the final time-step, and has a magnitude of 1. Notice how quickly the return decays with even a discount factor 0.9. This makes the agent quite short sighted. Due to its exponential decay small discount factors often do not make sense.

\noindent
\\ Optimizing for the return is still dependant on the quality of the rewards. Ideally, rewards should represent the thing we want the agent to achieve. Since they are the only information the agent can learn from should also not be too sparse. The problem of dealing with environments which give sparse reward is a large hurdle in modern day RL, some solutions to this are addressed in \citepg{491}. In such environments it turns out to be useful to modify the rewards received by setting baselines, creating "intrinsic rewards", those could for example be rewards for exploring the environment and many others. 

\subsection{Simple Solution for Grid-World using the presented concepts}
TODO


